

@inproceedings{10.1145/3097983.3098036,
author = {Dong, Yuxiao and Chawla, Nitesh V. and Swami, Ananthram},
title = {Metapath2vec: Scalable Representation Learning for Heterogeneous Networks},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098036},
doi = {10.1145/3097983.3098036},
abstract = {We study the problem of representation learning in heterogeneous networks. Its unique challenges come from the existence of multiple types of nodes and links, which limit the feasibility of the conventional network embedding techniques. We develop two scalable representation learning models, namely metapath2vec and metapath2vec++. The metapath2vec model formalizes meta-path-based random walks to construct the heterogeneous neighborhood of a node and then leverages a heterogeneous skip-gram model to perform node embeddings. The metapath2vec++ model further enables the simultaneous modeling of structural and semantic correlations in heterogeneous networks. Extensive experiments show that metapath2vec and metapath2vec++ are able to not only outperform state-of-the-art embedding models in various heterogeneous network mining tasks, such as node classification, clustering, and similarity search, but also discern the structural and semantic correlations between diverse network objects.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {135–144},
numpages = {10},
keywords = {feature learning, heterogeneous information networks, heterogeneous representation learning, latent representations, network embedding},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@ARTICLE{2019arXiv190507790Z,
       author = {{Zhelezniak}, Vitalii and {Savkov}, Aleksandar and {Shen}, April and {Hammerla}, Nils Y.},
        title = "{Correlation Coefficients and Semantic Textual Similarity}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2019,
        month = may,
          eid = {arXiv:1905.07790},
        pages = {arXiv:1905.07790},
archivePrefix = {arXiv},
       eprint = {1905.07790},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190507790Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@INPROCEEDINGS{5992571,
  author={Y. {Sun} and R. {Barber} and M. {Gupta} and C. C. {Aggarwal} and J. {Han}},
  booktitle={2011 International Conference on Advances in Social Networks Analysis and Mining}, 
  title={Co-author Relationship Prediction in Heterogeneous Bibliographic Networks}, 
  year={2011},
  volume={},
  number={},
  pages={121-128},
  doi={10.1109/ASONAM.2011.112}}


@inproceedings{10.5555/2999792.2999959,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {Distributed Representations of Words and Phrases and Their Compositionality},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3111–3119},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

